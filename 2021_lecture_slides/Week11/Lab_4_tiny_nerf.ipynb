{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Lab_4_tiny_nerf.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "venv-tf115",
      "language": "python",
      "name": "venv-tf115"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLDTVWKq7-ei"
      },
      "source": [
        "<img src=\"https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e700ef6067b43821ed52768_pipeline_website-01.png\" width=\"800\">\n",
        "\n",
        "# Training a (tiny) NeRF\n",
        "\n",
        "In this Lab session, we will train a [Neural Radiance Field (NeRF)](http://www.matthewtancik.com/nerf) which learns a volumetric representation of a scene from a sparse set of viewpoint images. Make sure you read the abstract of [the original paper](https://arxiv.org/pdf/2003.08934.pdf) to get a basic idea of the NeRF concepts.\n",
        "\n",
        "Note that this code is a simplified version of [the full code](github.com/bmild/nerf) and has been adapted from the Tiny NeRF notebook provided by the authors on their github page, e.g. this notebook does not include the 5D input taking into account view directions or hierarchical sampling.\n",
        "\n",
        "In this Lab session, we will cover the same training steps as in previous labs.\n",
        "\n",
        "We will do the following steps in order:\n",
        "\n",
        "1. Import your Libraries\n",
        "2. Load the Dataset\n",
        "3. Visualize the Data\n",
        "4. Define the Network\n",
        "5. Define the Optimizer\n",
        "6. Optimize the Network\n",
        "7. Visualise the Results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJR782sew2e3"
      },
      "source": [
        "# Step 1: Import the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZNXlxmEj0FC"
      },
      "source": [
        "# Unlike previous labs, this notebook is based on TensorFlow.\n",
        "# While specific syntax elements will be different, you should find some similarities\n",
        "# e.g. in the way batch of images are handled.\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    %tensorflow_version 1.x\n",
        "\n",
        "import os, sys\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCFbJxE5hMVG"
      },
      "source": [
        "# Step 2:  Load the Data (Images and Poses)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mTxAwgrj4yn"
      },
      "source": [
        "if not os.path.exists('tiny_nerf_data.npz'):\n",
        "    !wget http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz\n",
        "data = np.load('tiny_nerf_data.npz')\n",
        "\n",
        "images = data['images']\n",
        "poses = data['poses']\n",
        "focal = data['focal']\n",
        "H, W = images.shape[1:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_47n9-1li216"
      },
      "source": [
        "# Step 3: Visualize the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YJ3Sj7hDFTe"
      },
      "source": [
        "# Find the length of the dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8l84jnWE__E"
      },
      "source": [
        "# Examine the first data point in the dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfCW1bNyJnKu"
      },
      "source": [
        "# Examine the next data point in the dataset.\n",
        "# What do you notice about the two images and their poses?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIxhewothMVJ"
      },
      "source": [
        "## Display a sample grid of images\n",
        "Let us show some of the training images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwXUBeFWGvpC"
      },
      "source": [
        "# Display a grid of the input images with their corresponding poses."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYfIuQFlTwbR"
      },
      "source": [
        "## Split the training and validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHZZflB1zavT"
      },
      "source": [
        "# Clip the final six data points and use the first clipped data point\n",
        "# as the test point\n",
        "testimg, testpose = images[101], poses[101]\n",
        "images = images[:100,...,:3]\n",
        "poses = poses[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBh9GhJdhMVL"
      },
      "source": [
        "# Step 4: Define the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1avtwVoAQTu"
      },
      "source": [
        "def posenc(x):\n",
        "  rets = [x]\n",
        "  for i in range(L_embed):\n",
        "    for fn in [tf.sin, tf.cos]:\n",
        "      rets.append(fn(2.**i * x))\n",
        "  return tf.concat(rets, -1)\n",
        "\n",
        "# TODO: try switching the input embedding by commenting/uncommenting the lines below\n",
        "# and retrain the NeRF. What can you observe?\n",
        "L_embed = 6\n",
        "embed_fn = posenc\n",
        "# L_embed = 0\n",
        "# embed_fn = tf.identity\n",
        "\n",
        "def init_model(D=8, W=256):\n",
        "    relu = tf.keras.layers.ReLU()    \n",
        "    dense = lambda W=W, act=relu : tf.keras.layers.Dense(W, activation=act)\n",
        "\n",
        "    inputs = tf.keras.Input(shape=(3 + 3*2*L_embed)) \n",
        "    outputs = inputs\n",
        "    for i in range(D):\n",
        "        outputs = dense()(outputs)\n",
        "        if i%4==0 and i>0:\n",
        "            outputs = tf.concat([outputs, inputs], -1)\n",
        "    outputs = dense(4, act=None)(outputs)\n",
        "    \n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "def get_rays(H, W, focal, c2w):\n",
        "    i, j = tf.meshgrid(tf.range(W, dtype=tf.float32), tf.range(H, dtype=tf.float32), indexing='xy')\n",
        "    dirs = tf.stack([(i-W*.5)/focal, -(j-H*.5)/focal, -tf.ones_like(i)], -1)\n",
        "    rays_d = tf.reduce_sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)\n",
        "    rays_o = tf.broadcast_to(c2w[:3,-1], tf.shape(rays_d))\n",
        "    return rays_o, rays_d\n",
        "\n",
        "def render_rays(network_fn, rays_o, rays_d, near, far, N_samples, rand=False):\n",
        "\n",
        "    def batchify(fn, chunk=1024*32):\n",
        "        return lambda inputs : tf.concat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n",
        "    \n",
        "    # Compute 3D query points\n",
        "    z_vals = tf.linspace(near, far, N_samples) \n",
        "    if rand:\n",
        "      z_vals += tf.random.uniform(list(rays_o.shape[:-1]) + [N_samples]) * (far-near)/N_samples\n",
        "    pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None]\n",
        "    \n",
        "    # Run network\n",
        "    pts_flat = tf.reshape(pts, [-1,3])\n",
        "    pts_flat = embed_fn(pts_flat)\n",
        "    raw = batchify(network_fn)(pts_flat)\n",
        "    raw = tf.reshape(raw, list(pts.shape[:-1]) + [4])\n",
        "    \n",
        "    # Compute opacities and colors\n",
        "    sigma_a = tf.nn.relu(raw[...,3])\n",
        "    rgb = tf.math.sigmoid(raw[...,:3]) \n",
        "    \n",
        "    # Do volume rendering\n",
        "    dists = tf.concat([z_vals[..., 1:] - z_vals[..., :-1], tf.broadcast_to([1e10], z_vals[...,:1].shape)], -1) \n",
        "    alpha = 1.-tf.exp(-sigma_a * dists)  \n",
        "    weights = alpha * tf.math.cumprod(1.-alpha + 1e-10, -1, exclusive=True)\n",
        "    \n",
        "    rgb_map = tf.reduce_sum(weights[...,None] * rgb, -2) \n",
        "    depth_map = tf.reduce_sum(weights * z_vals, -1) \n",
        "    acc_map = tf.reduce_sum(weights, -1)\n",
        "\n",
        "    return rgb_map, depth_map, acc_map\n",
        "\n",
        "# Define network\n",
        "model = init_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-D1l0z9hMVO"
      },
      "source": [
        "# Step 5: Define the Optimizer\n",
        "\n",
        "Let's use the Adam Optimizer with a learning rate of $5\\times10^{-4}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4guHCE6GIIBn"
      },
      "source": [
        "# Define the optimizer \n",
        "optimizer = tf.keras.optimizers.Adam(5e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxDt192E-v6i"
      },
      "source": [
        "# Step 6: Optimize the Network\n",
        "\n",
        "Here we optimize the model. We plot a rendered holdout view and its PSNR every 25 iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XurcHoCj0FQ"
      },
      "source": [
        "N_samples = 64\n",
        "N_iters = 1000\n",
        "psnrs = []\n",
        "iternums = []\n",
        "i_plot = 25\n",
        "\n",
        "import time\n",
        "t = time.time()\n",
        "for i in range(N_iters+1):\n",
        "    \n",
        "    img_i = np.random.randint(images.shape[0])\n",
        "    target = images[img_i]\n",
        "    pose = poses[img_i]\n",
        "    rays_o, rays_d = get_rays(H, W, focal, pose)\n",
        "    with tf.GradientTape() as tape:\n",
        "        rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=N_samples, rand=True)\n",
        "        loss = tf.reduce_mean(tf.square(rgb - target))\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    \n",
        "    if i%i_plot==0:\n",
        "        print(i, (time.time() - t) / i_plot, 'secs per iter')\n",
        "        t = time.time()\n",
        "        \n",
        "        # Render the holdout view for logging\n",
        "        rays_o, rays_d = get_rays(H, W, focal, testpose)\n",
        "        rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=N_samples)\n",
        "        loss = tf.reduce_mean(tf.square(rgb - testimg))\n",
        "        psnr = -10. * tf.math.log(loss) / tf.math.log(10.)\n",
        "\n",
        "        psnrs.append(psnr.numpy())\n",
        "        iternums.append(i)\n",
        "        \n",
        "        plt.figure(figsize=(10,4))\n",
        "        plt.subplot(121)\n",
        "        plt.imshow(rgb)\n",
        "        plt.title(f'Iteration: {i}')\n",
        "        plt.subplot(122)\n",
        "        plt.plot(iternums, psnrs)\n",
        "        plt.title('PSNR')\n",
        "        plt.show()\n",
        "\n",
        "print('Done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFWh7D86U0bK"
      },
      "source": [
        "# Step 7: Visualise the Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZLEFNox_UVK"
      },
      "source": [
        "## Interactive Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L92jHDI7j0FT"
      },
      "source": [
        "%matplotlib inline\n",
        "from ipywidgets import interactive, widgets\n",
        "\n",
        "\n",
        "trans_t = lambda t : tf.convert_to_tensor([\n",
        "    [1,0,0,0],\n",
        "    [0,1,0,0],\n",
        "    [0,0,1,t],\n",
        "    [0,0,0,1],\n",
        "], dtype=tf.float32)\n",
        "\n",
        "rot_phi = lambda phi : tf.convert_to_tensor([\n",
        "    [1,0,0,0],\n",
        "    [0,tf.cos(phi),-tf.sin(phi),0],\n",
        "    [0,tf.sin(phi), tf.cos(phi),0],\n",
        "    [0,0,0,1],\n",
        "], dtype=tf.float32)\n",
        "\n",
        "rot_theta = lambda th : tf.convert_to_tensor([\n",
        "    [tf.cos(th),0,-tf.sin(th),0],\n",
        "    [0,1,0,0],\n",
        "    [tf.sin(th),0, tf.cos(th),0],\n",
        "    [0,0,0,1],\n",
        "], dtype=tf.float32)\n",
        "\n",
        "\n",
        "def pose_spherical(theta, phi, radius):\n",
        "    c2w = trans_t(radius)\n",
        "    c2w = rot_phi(phi/180.*np.pi) @ c2w\n",
        "    c2w = rot_theta(theta/180.*np.pi) @ c2w\n",
        "    c2w = np.array([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]]) @ c2w\n",
        "    return c2w\n",
        "\n",
        "\n",
        "def f(**kwargs):\n",
        "    c2w = pose_spherical(**kwargs)\n",
        "    rays_o, rays_d = get_rays(H, W, focal, c2w[:3,:4])\n",
        "    rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=N_samples)\n",
        "    img = np.clip(rgb,0,1)\n",
        "    \n",
        "    plt.figure(2, figsize=(20,6))\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "sldr = lambda v, mi, ma: widgets.FloatSlider(\n",
        "    value=v,\n",
        "    min=mi,\n",
        "    max=ma,\n",
        "    step=.01,\n",
        ")\n",
        "\n",
        "names = [\n",
        "    ['theta', [100., 0., 360]],\n",
        "    ['phi', [-30., -90, 0]],\n",
        "    ['radius', [4., 3., 5.]],\n",
        "]\n",
        "\n",
        "interactive_plot = interactive(f, **{s[0] : sldr(*s[1]) for s in names})\n",
        "output = interactive_plot.children[-1]\n",
        "output.layout.height = '350px'\n",
        "interactive_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpKhAn2a__Iu"
      },
      "source": [
        "## Render 360 Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sg4aV0cmVPs"
      },
      "source": [
        "frames = []\n",
        "for th in tqdm(np.linspace(0., 360., 120, endpoint=False)):\n",
        "    c2w = pose_spherical(th, -30., 4.)\n",
        "    rays_o, rays_d = get_rays(H, W, focal, c2w[:3,:4])\n",
        "    rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=N_samples)\n",
        "    frames.append((255*np.clip(rgb,0,1)).astype(np.uint8))\n",
        "\n",
        "import imageio\n",
        "f = 'video.mp4'\n",
        "imageio.mimwrite(f, frames, fps=30, quality=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ_ms-YMyFly"
      },
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls autoplay loop>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZENKx4PPeOI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}